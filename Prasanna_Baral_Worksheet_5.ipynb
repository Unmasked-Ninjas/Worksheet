{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4fW3ThS8G6Xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c4b009-827e-47e3-ee23-c8578cbef1c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Dataset/student.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying top and bottom 5 of the datasets\n",
        "print(df.head())\n",
        "print(df.tail())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl3i8h4MIPgo",
        "outputId": "18929117-c3b6-43e2-9e1f-f149b7d0bc82"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing info\n",
        "print(df.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrZl8U17IfbK",
        "outputId": "bb45281b-ff95-404a-ccc5-e638f58fd5fd"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descriptive info about the Dataset.\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fyu2RWAIpU6",
        "outputId": "3590d05f-8918-4698-ee96-f06e8eb6179c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Split your data into Feature (X) and Label (Y)\n",
        "X = df[['Math','Reading']].values\n",
        "Y = df['Writing'].values\n",
        "print(\"Feature of X\\n\",X)\n",
        "print(\"Label of y\\n\",Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRTsTWibI0S3",
        "outputId": "556f2668-e3da-48e1-ec1f-d691a3cae4b2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature of X\n",
            " [[48 68]\n",
            " [62 81]\n",
            " [79 80]\n",
            " ...\n",
            " [89 87]\n",
            " [83 82]\n",
            " [66 66]]\n",
            "Label of y\n",
            " [ 63  72  78  79  62  85  83  41  80  77  64  90  45  77  70  46  76  44\n",
            "  85  72  53  66  75  49  84  83  68  66  77  78  74  83  72  65  46  66\n",
            "  50  79  68  46  86  70  61  53  72  75  50  77 100  81 100  87  78  48\n",
            "  50  44  48  43  67  78  58  91  92  78  42  85  73  83  61  58  60  55\n",
            "  48  62  68  59  62  48  74  63  80  79  73  79  45  67  89  77  81  88\n",
            "  53  68  79  77  63  73  60  67 100  79  26  51  80  57  41  78  68  49\n",
            "  76  41  71  77  89  86  55  80  56  74  85  80  73  74  86  56  53  44\n",
            "  41  59  71  81  74  78  67  53  56  75  82  79  99  76  59  96  75  61\n",
            "  56  88  65 100  79  55  61  83  74  59  54  47  82  74  59  74  84  59\n",
            "  43  65  61  78  84  73  73  92  63  72  61  59  70  87  78  65  73  62\n",
            "  69  55  73  63  67  86  78  85  83  80  60  90  56  70  55  80  82  60\n",
            "  78  76  94  75  68  71  85  46  58  46  84  58  57  59  77  63  68  99\n",
            "  48  91  57  80  46  75  59  87  82  79  66  68  66  61  66  63  72  73\n",
            "  77  84  83  42  72  76  76  39  74  43  63  74  52  31  65  45  87  63\n",
            "  51  82  86  76  27  70  79  66  61  62  47  17  65  76  75  66  59  61\n",
            "  93  40  66  43  71  64  55  86  65  70  65  53  49  67  76  95  76  48\n",
            "  60  53  69  78  62  66  51  52  46  42  77  57 100  84  68  48  72  50\n",
            "  72  55  72  77  56  94  67  82  75  80  60  73  74  62  53  69  75  60\n",
            "  58  71  87  74  87  73  78  76  74  55  94  71  76  59  91  57  83  59\n",
            "  93  64  58  79  96  76  64  70  80  33  95  64  92  34  72  81  57  79\n",
            "  84  82  54  45  54  62  49  74  59  63  83  62  72  72  65  65  54  78\n",
            "  82  85  74  83  71  83  77  66  75  52  68  84  67  70  41  91  46  58\n",
            "  67  70  83  64 100  49  77  57  67  80  74  41  67  59  86  88  57  80\n",
            "  58  52  31  84  97  71  62  58  71  41  66 100  51  35  81  94  72  38\n",
            "  82  79  55  75  90  95  65  39  85  86  54  93  69  84  78  58  73  60\n",
            "  44  67  69  55  59  88  42  78  84  68  66  51  43  38  69  90  73  67\n",
            "  57  81  63  80  78  65  74  80  60  60  63  64  72  51  71  63  82  76\n",
            "  39  79  48  70  90  73  58 100  80  75  72  79  52  56  65  45  59  61\n",
            "  47  62  83  90  76  72  69  57  56  40  79  48  57  47  78  45  74  69\n",
            "  59  85  45  54  72  74  75  55  49  53  83  22 100  67  83  46  43  74\n",
            "  64  35  67  87  77  91  74  96  82  78  73  52  91  66  67  71  74  71\n",
            "  61  47  76  85  93  41  81  86  53  91  68  96  48  71  75  72  71  62\n",
            "  67  53  74  63  82  57  69  52  91  73  73  75  36  71  62 100  50  74\n",
            "  60  75  83  83 100  67  71  77  67  95  52  71  74  60  67  79  75  95\n",
            "  69  80  48  61  82  39  70  70  69  32  79  53  59  83 100  80  80  82\n",
            "  56  83  85  88  81  95  63  70  89  59  56  62  95  63  82  69  58  74\n",
            "  66  82  94  70  78  63  91  70  62  79  65  74  56  65 100  70  66  54\n",
            "  72  90  56  65  50  95  38  76  84  76  55  85  70  73  80  83  53  67\n",
            " 100  67  44  96  48  77 100  40  91  55  41  25  63  59  63  77  46  49\n",
            "  46  93  39  58  87  57  77 100  65  34  87  81  63  69  74  70  93  63\n",
            "  81  81  63  87  76  54  89  63  76  79  75  50  36  82  83  85  82  41\n",
            "  82  45  57  88  81  98  61  95  84  71  52  71  90  75  62  63  86  70\n",
            "  77  68  80  67  67  89  60  79  80  78  70  72  43  14  54  92  71  65\n",
            "  58  56  67  64  81  55  45  86  52  75  81  62  42  21  72  55  66  69\n",
            "  86  67  78  85  66  47 100  63  62  61  69  57  76  52  47  51  61  45\n",
            "  59  81  65  53  61  90  74  62  67  50  84  70  52  92  65  65  67  72\n",
            "  66  62  99  62  53  57  78  56  87  79  63  87  86  75  70  60  49  41\n",
            "  78  58  75  89  34  60  80  85  73  58  69  74  52  58  79  86  61  68\n",
            "  67  48  65  73  57  73  57  80  85  81  61  69 100  99  92  72  57  44\n",
            "  59  62  93  64  57  72  40  85  60  83  63  74  44  61  74  68  78  50\n",
            "  70  68  82  46  96 100  44  41  95  79  67  52  87  75  61  42  60  57\n",
            "  64  52  68  58  93  75  77  66  63  90  43  65  95  86  31  95  52  63\n",
            "  87  70  59  84  79  77  75  66  69  85  63  50  58  80  47  55  61  87\n",
            "  77  54  66  68  54  69  74  81  72  61  76  63  64  73  62  92  69  70\n",
            "  65  53  74  61  80  85  62  80  83  56  76  52  51  74  57  63  61  87\n",
            "  60  54  89  67  56  70  90  94  78  72]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(f\"Training data (X_train): {X_train.shape}\")\n",
        "print(f\"Test data (X_test): {X_test.shape}\")\n",
        "print(f\"Training labels (Y_train): {Y_train.shape}\")\n",
        "print(f\"Test labels (Y_test): {Y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paK0YS72PeTw",
        "outputId": "73d351c9-39bd-4a17-fe82-afcf935c9ed8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data (X_train): (800, 2)\n",
            "Test data (X_test): (200, 2)\n",
            "Training labels (Y_train): (800,)\n",
            "Test labels (Y_test): (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    This function finds the Mean Squared Error.\n",
        "    Input parameters:\n",
        "    X: Feature Matrix\n",
        "    Y: Target Matrix\n",
        "    W: Weight Matrix\n",
        "    Output Parameters:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "    # Calculate predictions using Y_pred = W^T * X\n",
        "    Y_pred = np.dot(X, W)\n",
        "\n",
        "    # Calculate the errors\n",
        "    errors = Y_pred - Y\n",
        "\n",
        "    # Compute the Mean Squared Error\n",
        "    cost = np.mean(errors ** 2)\n",
        "\n",
        "    return cost\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEGYAWgURZhK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training data (X_train): {X_train.shape}\")\n",
        "print(f\"Test data (X_test): {X_test.shape}\")\n",
        "print(f\"Training labels (Y_train): {Y_train.shape}\")\n",
        "print(f\"Test labels (Y_test): {Y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkTyZwkkRxII",
        "outputId": "e694cdf6-ea4e-4340-de68-8dd3f6974022"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data (X_train): (800, 2)\n",
            "Test data (X_test): (200, 2)\n",
            "Training labels (Y_train): (800,)\n",
            "Test labels (Y_test): (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.zeros(X_train.shape[1]) # Initialize weights\n",
        "alpha = 0.00001 # Learning rate\n",
        "iterations = 1000\n",
        "\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "Parameters:\n",
        "X (numpy.ndarray): Feature matrix (m x n).\n",
        "Y (numpy.ndarray): Target vector (m x 1).\n",
        "W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "alpha (float): Learning rate.\n",
        "iterations (int): Number of iterations for gradient descent.\n",
        "Returns:\n",
        "tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values\n",
        ".\n",
        "W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        "# Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        "# Number of samples\n",
        "  m = len(Y)\n",
        " #  # Copy the initial weights\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "  # Step 1: Hypothesis Values\n",
        "    Y_pred = np.dot(X, W)\n",
        "  # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y\n",
        "  # Step 3: Gradient Calculation\n",
        "    dw = np.dot(X.T, loss) / m\n",
        "  # Step 4: Updating Values of W using Gradient\n",
        "    W_update = W - alpha * dw\n",
        "    # Step 5: New Cost Value\n",
        "    cost = cost_function(X, Y, W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history"
      ],
      "metadata": {
        "id": "woccxaZfSCh-"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "Y_pred = np.dot(X_test, W_optimal)\n",
        "print(\"Predicted values:\", Y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMVGO6MjUhtR",
        "outputId": "04dc2801-cc5b-47ab-f3af-3136696dd7bb"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values: [6.48657161 3.87256319 6.42302765 6.27242169 9.41641535 6.39283863\n",
            " 9.28804601 4.33327573 6.27464535 8.47591956 8.15786059 6.33818931\n",
            " 6.91837654 4.92680493 6.22444336 5.92990243 6.97080219 5.91083171\n",
            " 8.16230791 6.62161194 7.52302405 6.54566825 6.76777058 7.74512638\n",
            " 5.98010441 8.01614928 5.10759991 6.4319223  8.24936991 4.65578203\n",
            " 9.33157701 8.5350162  3.5255966  5.35193886 8.58966551 6.83798553\n",
            " 9.02591776 7.96149996 6.78778354 7.42484374 8.1334003  5.46346115\n",
            " 5.50921581 6.58919925 5.93657341 8.0463383  7.09566645 7.76958666\n",
            " 4.02094549 7.71049003 4.7739753  6.44748794 6.28354    6.63050659\n",
            " 5.63313783 7.27201411 3.8223612  6.43859329 7.12140815 9.07389609\n",
            " 4.71710233 4.76508065 6.56473896 5.6642691  6.08940304 9.21783106\n",
            " 9.48218298 7.20847015 6.32484734 8.1076586  4.46386873 9.09613271\n",
            " 5.84728775 7.76069201 6.57585728 5.75577843 8.53056888 4.2239771\n",
            " 6.62828293 4.56204904 7.41372543 6.10052135 5.9867754  8.77268416\n",
            " 6.24667999 7.90907431 8.09876395 5.60517246 5.80598041 6.25557464\n",
            " 6.63495391 6.8157489  6.31817635 5.89526608 2.80592173 8.25381724\n",
            " 6.89263484 7.32443976 7.62120436 7.2608958  7.75402103 3.52337294\n",
            " 8.04856196 3.54560956 9.66742529 8.56965255 6.67848491 7.27423778\n",
            " 6.20092533 6.18313603 7.06453518 9.1898657  7.79532836 7.61453338\n",
            " 7.82201231 7.88461403 5.9610337  6.76999424 9.57813963 5.44567185\n",
            " 6.53899726 9.28582235 7.86331965 6.28576366 3.69399186 4.61002736\n",
            " 5.35638619 7.75846835 6.18980701 5.41770649 6.86150356 8.73360049\n",
            " 8.82288615 7.16938648 5.5416285  5.00274861 6.63273025 9.56702131\n",
            " 7.78070498 4.9045683  6.34041298 3.09601534 8.8753118  4.49055268\n",
            " 7.06231151 6.66959026 6.82686721 6.04587204 6.22666703 6.57140995\n",
            " 5.80820408 5.47013214 7.80867034 8.68784583 6.68293224 5.2883949\n",
            " 8.24936991 5.45679016 5.63980881 8.25159358 5.99789371 6.7900072\n",
            " 8.09654029 8.35866854 3.94055448 6.02808274 7.65806438 7.23293044\n",
            " 7.36797076 5.30840786 8.62652552 5.1142709  6.29688198 8.64876215\n",
            " 4.66467668 4.69708936 7.77403399 6.32929466 7.54431843 7.16493915\n",
            " 8.48703788 5.57848851 7.0689825  7.51190574 4.42033773 8.2382516\n",
            " 3.2901523  6.51676064 6.8770692  7.74512638 5.83172211 5.16892021\n",
            " 5.22356953 7.65139339 5.01831425 6.05254303 6.18980701 7.2173648\n",
            " 8.56298156 7.79755203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating root mean squared error\n",
        "\n",
        "\n",
        "def rmse(Y_test, Y_pred):\n",
        "  residuals = Y_test - Y_pred\n",
        "  squared_residuals = residuals ** 2\n",
        "  mean_squared_residuals = np.mean(squared_residuals)\n",
        "  rmse = np.sqrt(mean_squared_residuals)\n",
        "  return rmse\n",
        "\n",
        "rmse_value = rmse(Y_test, Y_pred)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O04LKa1qVBn8",
        "outputId": "a8d87e64-9f52-4b95-a01e-16a724046f9e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 63.36563528655014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating r2\n",
        "\n",
        "def r2(Y,Y_pred):\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "  ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res / ss_tot)\n",
        "  return r2\n",
        "\n",
        "\n",
        "\n",
        "model_r2 = r2(Y_test, Y_pred)\n",
        "# Step 8: Output the results\n",
        "print(\"Final Weights:\", W_optimal)\n",
        "print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "print(\"RMSE on Test Set:\", rmse_value)\n",
        "print(\"R-Squared on Test Set:\", model_r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pp5DbxpVlf-",
        "outputId": "a6fede07-fecb-44b1-ad87-5feee652c7f6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.04797833 0.05020199]\n",
            "Cost History (First 10 iterations): [4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751, 4026.33114156751]\n",
            "RMSE on Test Set: 63.36563528655014\n",
            "R-Squared on Test Set: -15.04041794561271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Dataset/student.csv')  # Ensure the correct file path\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values  # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.0001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vNk-UqSWriE",
        "outputId": "71685f66-9f94-43e0-9ce4-95ab19e0f9dd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.47978325 0.50201988]\n",
            "Cost History (First 10 iterations): [35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422, 35.62759435504422]\n",
            "RMSE on Test Set: 6.092665443799174\n",
            "R-Squared on Test Set: 0.851706281452247\n"
          ]
        }
      ]
    }
  ]
}